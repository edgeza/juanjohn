
#!/usr/bin/env python3
"""
Local Analysis Results Ingestion System

This system reads the JSON results files generated by the local analysis engine
and ingests them into the PostgreSQL database for the main application to use.

This bridges the gap between local processing and cloud-based display.
"""

import os
import json
import logging
import psycopg2
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import glob
from psycopg2.extras import RealDictCursor

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s:%(message)s',
    handlers=[
        logging.FileHandler("ingestion_system.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class IngestionSystem:
    def __init__(self, db_config: Dict, results_dir: str = "results"):
        """
        Initialize the ingestion system
        
        Args:
            db_config: PostgreSQL database configuration
            results_dir: Directory containing analysis results
        """
        self.db_config = db_config
        self.results_dir = results_dir
        self.connection = None
        
    def connect_database(self):
        """Establish database connection"""
        try:
            self.connection = psycopg2.connect(
                host=self.db_config['host'],
                port=self.db_config['port'],
                database=self.db_config['database'],
                user=self.db_config['user'],
                password=self.db_config['password']
            )
            logger.info("Database connection established")
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            raise
    
    def close_database(self):
        """Close database connection"""
        if self.connection:
            self.connection.close()
            logger.info("Database connection closed")
    
    def get_latest_results_file(self) -> Optional[str]:
        """
        Get the most recent results file
        
        Returns:
            Path to the latest results file, or None if none found
        """
        try:
            pattern = os.path.join(self.results_dir, "enhanced_analysis_results_*.json")
            files = glob.glob(pattern)
            
            if not files:
                logger.warning(f"No results files found in {self.results_dir}")
                return None
            
            # Sort by modification time (newest first)
            files.sort(key=os.path.getmtime, reverse=True)
            latest_file = files[0]
            
            logger.info(f"Found latest results file: {latest_file}")
            return latest_file
            
        except Exception as e:
            logger.error(f"Error finding latest results file: {e}")
            return None
    
    def load_results(self, filepath: str) -> Dict:
        """
        Load analysis results from JSON file
        
        Args:
            filepath: Path to results file
        
        Returns:
            Dictionary containing analysis results
        """
        try:
            with open(filepath, 'r') as f:
                results = json.load(f)
            
            logger.info(f"Loaded results from {filepath}")
            return results
            
        except Exception as e:
            logger.error(f"Error loading results from {filepath}: {e}")
            return {}
    
    def create_tables_if_not_exist(self):
        """Create necessary tables if they don't exist"""
        try:
            with self.connection.cursor() as cursor:
                # Create alerts table
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS trading.alerts (
                        id SERIAL PRIMARY KEY,
                        symbol VARCHAR(20) NOT NULL,
                        interval VARCHAR(10) NOT NULL,
                        signal VARCHAR(10) NOT NULL,
                        current_price NUMERIC(20,8),
                        upper_band NUMERIC(20,8),
                        lower_band NUMERIC(20,8),
                        potential_return NUMERIC(10,4),
                        signal_strength NUMERIC(5,2),
                        risk_level VARCHAR(10),
                        confirmations TEXT[],
                        technical_indicators JSONB,
                        polynomial_regression JSONB,
                        analysis_date TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    )
                """)
                
                # Create analysis_summary table
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS trading.analysis_summary (
                        id SERIAL PRIMARY KEY,
                        analysis_date TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        total_assets_analyzed INTEGER,
                        buy_signals INTEGER,
                        sell_signals INTEGER,
                        hold_signals INTEGER,
                        avg_potential_return NUMERIC(10,4),
                        high_risk_assets INTEGER,
                        correlation_analysis JSONB,
                        top_buy_signals JSONB,
                        top_sell_signals JSONB,
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    )
                """)
                
                # Create asset_analytics table for detailed analytics
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS trading.asset_analytics (
                        id SERIAL PRIMARY KEY,
                        symbol VARCHAR(20) NOT NULL,
                        analysis_date TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        current_price NUMERIC(20,8),
                        price_change_24h NUMERIC(10,4),
                        volatility NUMERIC(10,4),
                        trend VARCHAR(10),
                        support_level NUMERIC(20,8),
                        resistance_level NUMERIC(20,8),
                        volume_ratio NUMERIC(10,4),
                        technical_indicators JSONB,
                        polynomial_regression JSONB,
                        signal_analysis JSONB,
                        data_points INTEGER,
                        analysis_period_days INTEGER,
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    )
                """)
                
                self.connection.commit()
                logger.info("Tables created/verified successfully")
                
        except Exception as e:
            logger.error(f"Error creating tables: {e}")
            self.connection.rollback()
            raise
    
    def ingest_alerts(self, results: Dict) -> int:
        """
        Ingest alert signals into the database
        
        Args:
            results: Analysis results dictionary
        
        Returns:
            Number of alerts ingested
        """
        try:
            ingested_count = 0
            
            if 'individual_analyses' not in results:
                logger.warning("No individual analyses found in results")
                return 0
            
            with self.connection.cursor() as cursor:
                for analysis in results['individual_analyses']:
                    if 'error' in analysis:
                        logger.warning(f"Skipping {analysis.get('symbol', 'unknown')}: {analysis['error']}")
                        continue
                    
                    symbol = analysis['symbol']
                    signal_analysis = analysis['signal_analysis']
                    
                    # Check if alert already exists for this symbol
                    cursor.execute("""
                        SELECT id FROM trading.alerts 
                        WHERE symbol = %s AND analysis_date::date = CURRENT_DATE
                    """, (symbol,))
                    
                    existing_alert = cursor.fetchone()
                    
                    if existing_alert:
                        # Update existing alert
                        cursor.execute("""
                            UPDATE trading.alerts SET
                                signal = %s,
                                current_price = %s,
                                upper_band = %s,
                                lower_band = %s,
                                potential_return = %s,
                                signal_strength = %s,
                                risk_level = %s,
                                confirmations = %s,
                                technical_indicators = %s,
                                polynomial_regression = %s,
                                updated_at = NOW()
                            WHERE id = %s
                        """, (
                            signal_analysis['signal'],
                            signal_analysis['current_price'],
                            signal_analysis['upper_band'],
                            signal_analysis['lower_band'],
                            signal_analysis['potential_return'],
                            signal_analysis['signal_strength'],
                            signal_analysis['risk_level'],
                            signal_analysis['confirmations'],
                            json.dumps(analysis['technical_indicators']),
                            json.dumps(analysis['polynomial_regression']),
                            existing_alert[0]
                        ))
                        logger.info(f"Updated alert for {symbol}")
                    else:
                        # Insert new alert
                        cursor.execute("""
                            INSERT INTO trading.alerts (
                                symbol, interval, signal, current_price, upper_band, lower_band,
                                potential_return, signal_strength, risk_level, confirmations,
                                technical_indicators, polynomial_regression
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """, (
                            symbol,
                            '1d',  # Default interval
                            signal_analysis['signal'],
                            signal_analysis['current_price'],
                            signal_analysis['upper_band'],
                            signal_analysis['lower_band'],
                            signal_analysis['potential_return'],
                            signal_analysis['signal_strength'],
                            signal_analysis['risk_level'],
                            signal_analysis['confirmations'],
                            json.dumps(analysis['technical_indicators']),
                            json.dumps(analysis['polynomial_regression'])
                        ))
                        logger.info(f"Inserted new alert for {symbol}")
                    
                    ingested_count += 1
            
            self.connection.commit()
            logger.info(f"Successfully ingested {ingested_count} alerts")
            return ingested_count
            
        except Exception as e:
            logger.error(f"Error ingesting alerts: {e}")
            self.connection.rollback()
            return 0
    
    def ingest_analysis_summary(self, results: Dict) -> bool:
        """
        Ingest analysis summary into the database
        
        Args:
            results: Analysis results dictionary
        
        Returns:
            True if successful, False otherwise
        """
        try:
            if 'summary' not in results:
                logger.warning("No summary found in results")
                return False
            
            summary = results['summary']
            
            with self.connection.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO trading.analysis_summary (
                        total_assets_analyzed, buy_signals, sell_signals, hold_signals,
                        avg_potential_return, high_risk_assets, correlation_analysis,
                        top_buy_signals, top_sell_signals
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    summary['total_assets_analyzed'],
                    summary['buy_signals'],
                    summary['sell_signals'],
                    summary['hold_signals'],
                    summary['avg_potential_return'],
                    summary['high_risk_assets'],
                    json.dumps(results.get('correlation_analysis', {})),
                    json.dumps(results.get('top_buy_signals', [])),
                    json.dumps(results.get('top_sell_signals', []))
                ))
                
                self.connection.commit()
                logger.info("Analysis summary ingested successfully")
                return True
                
        except Exception as e:
            logger.error(f"Error ingesting analysis summary: {e}")
            self.connection.rollback()
            return False
    
    def ingest_asset_analytics(self, results: Dict) -> int:
        """
        Ingest detailed asset analytics into the database
        
        Args:
            results: Analysis results dictionary
        
        Returns:
            Number of asset analytics ingested
        """
        try:
            ingested_count = 0
            
            if 'individual_analyses' not in results:
                logger.warning("No individual analyses found in results")
                return 0
            
            with self.connection.cursor() as cursor:
                for analysis in results['individual_analyses']:
                    if 'error' in analysis:
                        continue
                    
                    symbol = analysis['symbol']
                    
                    # Check if analytics already exists for this symbol today
                    cursor.execute("""
                        SELECT id FROM trading.asset_analytics 
                        WHERE symbol = %s AND analysis_date::date = CURRENT_DATE
                    """, (symbol,))
                    
                    existing_analytics = cursor.fetchone()
                    
                    if existing_analytics:
                        # Update existing analytics
                        cursor.execute("""
                            UPDATE trading.asset_analytics SET
                                current_price = %s,
                                price_change_24h = %s,
                                volatility = %s,
                                trend = %s,
                                support_level = %s,
                                resistance_level = %s,
                                volume_ratio = %s,
                                technical_indicators = %s,
                                polynomial_regression = %s,
                                signal_analysis = %s,
                                data_points = %s,
                                analysis_period_days = %s
                            WHERE id = %s
                        """, (
                            analysis['current_price'],
                            analysis['price_change_24h'],
                            analysis['volatility'],
                            analysis['trend'],
                            analysis['support_level'],
                            analysis['resistance_level'],
                            analysis['volume_ratio'],
                            json.dumps(analysis['technical_indicators']),
                            json.dumps(analysis['polynomial_regression']),
                            json.dumps(analysis['signal_analysis']),
                            analysis['data_points'],
                            analysis['analysis_period_days'],
                            existing_analytics[0]
                        ))
                        logger.info(f"Updated analytics for {symbol}")
                    else:
                        # Insert new analytics
                        cursor.execute("""
                            INSERT INTO trading.asset_analytics (
                                symbol, current_price, price_change_24h, volatility, trend,
                                support_level, resistance_level, volume_ratio, technical_indicators,
                                polynomial_regression, signal_analysis, data_points, analysis_period_days
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """, (
                            symbol,
                            analysis['current_price'],
                            analysis['price_change_24h'],
                            analysis['volatility'],
                            analysis['trend'],
                            analysis['support_level'],
                            analysis['resistance_level'],
                            analysis['volume_ratio'],
                            json.dumps(analysis['technical_indicators']),
                            json.dumps(analysis['polynomial_regression']),
                            json.dumps(analysis['signal_analysis']),
                            analysis['data_points'],
                            analysis['analysis_period_days']
                        ))
                        logger.info(f"Inserted new analytics for {symbol}")
                    
                    ingested_count += 1
            
            self.connection.commit()
            logger.info(f"Successfully ingested {ingested_count} asset analytics")
            return ingested_count
            
        except Exception as e:
            logger.error(f"Error ingesting asset analytics: {e}")
            self.connection.rollback()
            return 0
    
    def cleanup_old_data(self, days: int = 30):
        """
        Clean up old data to prevent database bloat
        
        Args:
            days: Number of days to keep data
        """
        try:
            with self.connection.cursor() as cursor:
                # Clean up old alerts
                cursor.execute("""
                    DELETE FROM trading.alerts 
                    WHERE created_at < NOW() - INTERVAL '%s days'
                """, (days,))
                alerts_deleted = cursor.rowcount
                
                # Clean up old analytics
                cursor.execute("""
                    DELETE FROM trading.asset_analytics 
                    WHERE created_at < NOW() - INTERVAL '%s days'
                """, (days,))
                analytics_deleted = cursor.rowcount
                
                # Clean up old summaries (keep only last 7 days)
                cursor.execute("""
                    DELETE FROM trading.analysis_summary 
                    WHERE created_at < NOW() - INTERVAL '7 days'
                """, ())
                summaries_deleted = cursor.rowcount
                
                self.connection.commit()
                logger.info(f"Cleanup completed: {alerts_deleted} alerts, {analytics_deleted} analytics, {summaries_deleted} summaries deleted")
                
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
            self.connection.rollback()
    
    def ingest_latest_results(self) -> Dict:
        """
        Ingest the latest analysis results
        
        Returns:
            Dictionary with ingestion results
        """
        try:
            # Connect to database
            self.connect_database()
            
            # Create tables if they don't exist
            self.create_tables_if_not_exist()
            
            # Get latest results file
            latest_file = self.get_latest_results_file()
            if not latest_file:
                return {'error': 'No results files found'}
            
            # Load results
            results = self.load_results(latest_file)
            if not results:
                return {'error': 'Failed to load results'}
            
            # Ingest data
            alerts_ingested = self.ingest_alerts(results)
            summary_ingested = self.ingest_analysis_summary(results)
            analytics_ingested = self.ingest_asset_analytics(results)
            
            # Cleanup old data
            self.cleanup_old_data()
            
            # Close database connection
            self.close_database()
            
            return {
                'success': True,
                'file_processed': latest_file,
                'alerts_ingested': alerts_ingested,
                'summary_ingested': summary_ingested,
                'analytics_ingested': analytics_ingested,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error during ingestion: {e}")
            if self.connection:
                self.close_database()
            return {'error': str(e)}

def main():
    """Main function for running the ingestion system"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Ingest local analysis results into database')
    parser.add_argument('--config', default='config.json', help='Configuration file path')
    parser.add_argument('--results-dir', default='results', help='Results directory path')
    parser.add_argument('--cleanup-days', type=int, default=30, help='Days to keep data')
    
    args = parser.parse_args()
    
    # Load configuration
    try:
        with open(args.config, 'r') as f:
            config = json.load(f)
    except Exception as e:
        print(f"Error loading config: {e}")
        return
    
    # Database configuration
    db_config = {
        'host': config.get('database_host', 'localhost'),
        'port': config.get('database_port', 5432),
        'database': config.get('database_name', 'autonama'),
        'user': config.get('database_user', 'postgres'),
        'password': config.get('database_password', 'postgres')
    }
    
    # Initialize and run ingestion
    ingestion_system = IngestionSystem(db_config, args.results_dir)
    results = ingestion_system.ingest_latest_results()
    
    if 'error' in results:
        print(f"‚ùå Ingestion failed: {results['error']}")
    else:
        print(f"‚úÖ Ingestion completed successfully!")
        print(f"üìÅ File processed: {results['file_processed']}")
        print(f"üìä Alerts ingested: {results['alerts_ingested']}")
        print(f"üìà Analytics ingested: {results['analytics_ingested']}")
        print(f"üìã Summary ingested: {results['summary_ingested']}")

if __name__ == "__main__":
    main() 
 
"""
Local Analysis Results Ingestion System

This system reads the JSON results files generated by the local analysis engine
and ingests them into the PostgreSQL database for the main application to use.

This bridges the gap between local processing and cloud-based display.
"""

import os
import json
import logging
import psycopg2
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any
import glob
from psycopg2.extras import RealDictCursor

# Set up logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s %(levelname)s:%(message)s',
    handlers=[
        logging.FileHandler("ingestion_system.log"),
        logging.StreamHandler()
    ]
)

logger = logging.getLogger(__name__)

class IngestionSystem:
    def __init__(self, db_config: Dict, results_dir: str = "results"):
        """
        Initialize the ingestion system
        
        Args:
            db_config: PostgreSQL database configuration
            results_dir: Directory containing analysis results
        """
        self.db_config = db_config
        self.results_dir = results_dir
        self.connection = None
        
    def connect_database(self):
        """Establish database connection"""
        try:
            self.connection = psycopg2.connect(
                host=self.db_config['host'],
                port=self.db_config['port'],
                database=self.db_config['database'],
                user=self.db_config['user'],
                password=self.db_config['password']
            )
            logger.info("Database connection established")
        except Exception as e:
            logger.error(f"Database connection failed: {e}")
            raise
    
    def close_database(self):
        """Close database connection"""
        if self.connection:
            self.connection.close()
            logger.info("Database connection closed")
    
    def get_latest_results_file(self) -> Optional[str]:
        """
        Get the most recent results file
        
        Returns:
            Path to the latest results file, or None if none found
        """
        try:
            pattern = os.path.join(self.results_dir, "enhanced_analysis_results_*.json")
            files = glob.glob(pattern)
            
            if not files:
                logger.warning(f"No results files found in {self.results_dir}")
                return None
            
            # Sort by modification time (newest first)
            files.sort(key=os.path.getmtime, reverse=True)
            latest_file = files[0]
            
            logger.info(f"Found latest results file: {latest_file}")
            return latest_file
            
        except Exception as e:
            logger.error(f"Error finding latest results file: {e}")
            return None
    
    def load_results(self, filepath: str) -> Dict:
        """
        Load analysis results from JSON file
        
        Args:
            filepath: Path to results file
        
        Returns:
            Dictionary containing analysis results
        """
        try:
            with open(filepath, 'r') as f:
                results = json.load(f)
            
            logger.info(f"Loaded results from {filepath}")
            return results
            
        except Exception as e:
            logger.error(f"Error loading results from {filepath}: {e}")
            return {}
    
    def create_tables_if_not_exist(self):
        """Create necessary tables if they don't exist"""
        try:
            with self.connection.cursor() as cursor:
                # Create alerts table
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS trading.alerts (
                        id SERIAL PRIMARY KEY,
                        symbol VARCHAR(20) NOT NULL,
                        interval VARCHAR(10) NOT NULL,
                        signal VARCHAR(10) NOT NULL,
                        current_price NUMERIC(20,8),
                        upper_band NUMERIC(20,8),
                        lower_band NUMERIC(20,8),
                        potential_return NUMERIC(10,4),
                        signal_strength NUMERIC(5,2),
                        risk_level VARCHAR(10),
                        confirmations TEXT[],
                        technical_indicators JSONB,
                        polynomial_regression JSONB,
                        analysis_date TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        updated_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    )
                """)
                
                # Create analysis_summary table
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS trading.analysis_summary (
                        id SERIAL PRIMARY KEY,
                        analysis_date TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        total_assets_analyzed INTEGER,
                        buy_signals INTEGER,
                        sell_signals INTEGER,
                        hold_signals INTEGER,
                        avg_potential_return NUMERIC(10,4),
                        high_risk_assets INTEGER,
                        correlation_analysis JSONB,
                        top_buy_signals JSONB,
                        top_sell_signals JSONB,
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    )
                """)
                
                # Create asset_analytics table for detailed analytics
                cursor.execute("""
                    CREATE TABLE IF NOT EXISTS trading.asset_analytics (
                        id SERIAL PRIMARY KEY,
                        symbol VARCHAR(20) NOT NULL,
                        analysis_date TIMESTAMP WITH TIME ZONE DEFAULT NOW(),
                        current_price NUMERIC(20,8),
                        price_change_24h NUMERIC(10,4),
                        volatility NUMERIC(10,4),
                        trend VARCHAR(10),
                        support_level NUMERIC(20,8),
                        resistance_level NUMERIC(20,8),
                        volume_ratio NUMERIC(10,4),
                        technical_indicators JSONB,
                        polynomial_regression JSONB,
                        signal_analysis JSONB,
                        data_points INTEGER,
                        analysis_period_days INTEGER,
                        created_at TIMESTAMP WITH TIME ZONE DEFAULT NOW()
                    )
                """)
                
                self.connection.commit()
                logger.info("Tables created/verified successfully")
                
        except Exception as e:
            logger.error(f"Error creating tables: {e}")
            self.connection.rollback()
            raise
    
    def ingest_alerts(self, results: Dict) -> int:
        """
        Ingest alert signals into the database
        
        Args:
            results: Analysis results dictionary
        
        Returns:
            Number of alerts ingested
        """
        try:
            ingested_count = 0
            
            if 'individual_analyses' not in results:
                logger.warning("No individual analyses found in results")
                return 0
            
            with self.connection.cursor() as cursor:
                for analysis in results['individual_analyses']:
                    if 'error' in analysis:
                        logger.warning(f"Skipping {analysis.get('symbol', 'unknown')}: {analysis['error']}")
                        continue
                    
                    symbol = analysis['symbol']
                    signal_analysis = analysis['signal_analysis']
                    
                    # Check if alert already exists for this symbol
                    cursor.execute("""
                        SELECT id FROM trading.alerts 
                        WHERE symbol = %s AND analysis_date::date = CURRENT_DATE
                    """, (symbol,))
                    
                    existing_alert = cursor.fetchone()
                    
                    if existing_alert:
                        # Update existing alert
                        cursor.execute("""
                            UPDATE trading.alerts SET
                                signal = %s,
                                current_price = %s,
                                upper_band = %s,
                                lower_band = %s,
                                potential_return = %s,
                                signal_strength = %s,
                                risk_level = %s,
                                confirmations = %s,
                                technical_indicators = %s,
                                polynomial_regression = %s,
                                updated_at = NOW()
                            WHERE id = %s
                        """, (
                            signal_analysis['signal'],
                            signal_analysis['current_price'],
                            signal_analysis['upper_band'],
                            signal_analysis['lower_band'],
                            signal_analysis['potential_return'],
                            signal_analysis['signal_strength'],
                            signal_analysis['risk_level'],
                            signal_analysis['confirmations'],
                            json.dumps(analysis['technical_indicators']),
                            json.dumps(analysis['polynomial_regression']),
                            existing_alert[0]
                        ))
                        logger.info(f"Updated alert for {symbol}")
                    else:
                        # Insert new alert
                        cursor.execute("""
                            INSERT INTO trading.alerts (
                                symbol, interval, signal, current_price, upper_band, lower_band,
                                potential_return, signal_strength, risk_level, confirmations,
                                technical_indicators, polynomial_regression
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """, (
                            symbol,
                            '1d',  # Default interval
                            signal_analysis['signal'],
                            signal_analysis['current_price'],
                            signal_analysis['upper_band'],
                            signal_analysis['lower_band'],
                            signal_analysis['potential_return'],
                            signal_analysis['signal_strength'],
                            signal_analysis['risk_level'],
                            signal_analysis['confirmations'],
                            json.dumps(analysis['technical_indicators']),
                            json.dumps(analysis['polynomial_regression'])
                        ))
                        logger.info(f"Inserted new alert for {symbol}")
                    
                    ingested_count += 1
            
            self.connection.commit()
            logger.info(f"Successfully ingested {ingested_count} alerts")
            return ingested_count
            
        except Exception as e:
            logger.error(f"Error ingesting alerts: {e}")
            self.connection.rollback()
            return 0
    
    def ingest_analysis_summary(self, results: Dict) -> bool:
        """
        Ingest analysis summary into the database
        
        Args:
            results: Analysis results dictionary
        
        Returns:
            True if successful, False otherwise
        """
        try:
            if 'summary' not in results:
                logger.warning("No summary found in results")
                return False
            
            summary = results['summary']
            
            with self.connection.cursor() as cursor:
                cursor.execute("""
                    INSERT INTO trading.analysis_summary (
                        total_assets_analyzed, buy_signals, sell_signals, hold_signals,
                        avg_potential_return, high_risk_assets, correlation_analysis,
                        top_buy_signals, top_sell_signals
                    ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s)
                """, (
                    summary['total_assets_analyzed'],
                    summary['buy_signals'],
                    summary['sell_signals'],
                    summary['hold_signals'],
                    summary['avg_potential_return'],
                    summary['high_risk_assets'],
                    json.dumps(results.get('correlation_analysis', {})),
                    json.dumps(results.get('top_buy_signals', [])),
                    json.dumps(results.get('top_sell_signals', []))
                ))
                
                self.connection.commit()
                logger.info("Analysis summary ingested successfully")
                return True
                
        except Exception as e:
            logger.error(f"Error ingesting analysis summary: {e}")
            self.connection.rollback()
            return False
    
    def ingest_asset_analytics(self, results: Dict) -> int:
        """
        Ingest detailed asset analytics into the database
        
        Args:
            results: Analysis results dictionary
        
        Returns:
            Number of asset analytics ingested
        """
        try:
            ingested_count = 0
            
            if 'individual_analyses' not in results:
                logger.warning("No individual analyses found in results")
                return 0
            
            with self.connection.cursor() as cursor:
                for analysis in results['individual_analyses']:
                    if 'error' in analysis:
                        continue
                    
                    symbol = analysis['symbol']
                    
                    # Check if analytics already exists for this symbol today
                    cursor.execute("""
                        SELECT id FROM trading.asset_analytics 
                        WHERE symbol = %s AND analysis_date::date = CURRENT_DATE
                    """, (symbol,))
                    
                    existing_analytics = cursor.fetchone()
                    
                    if existing_analytics:
                        # Update existing analytics
                        cursor.execute("""
                            UPDATE trading.asset_analytics SET
                                current_price = %s,
                                price_change_24h = %s,
                                volatility = %s,
                                trend = %s,
                                support_level = %s,
                                resistance_level = %s,
                                volume_ratio = %s,
                                technical_indicators = %s,
                                polynomial_regression = %s,
                                signal_analysis = %s,
                                data_points = %s,
                                analysis_period_days = %s
                            WHERE id = %s
                        """, (
                            analysis['current_price'],
                            analysis['price_change_24h'],
                            analysis['volatility'],
                            analysis['trend'],
                            analysis['support_level'],
                            analysis['resistance_level'],
                            analysis['volume_ratio'],
                            json.dumps(analysis['technical_indicators']),
                            json.dumps(analysis['polynomial_regression']),
                            json.dumps(analysis['signal_analysis']),
                            analysis['data_points'],
                            analysis['analysis_period_days'],
                            existing_analytics[0]
                        ))
                        logger.info(f"Updated analytics for {symbol}")
                    else:
                        # Insert new analytics
                        cursor.execute("""
                            INSERT INTO trading.asset_analytics (
                                symbol, current_price, price_change_24h, volatility, trend,
                                support_level, resistance_level, volume_ratio, technical_indicators,
                                polynomial_regression, signal_analysis, data_points, analysis_period_days
                            ) VALUES (%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s, %s)
                        """, (
                            symbol,
                            analysis['current_price'],
                            analysis['price_change_24h'],
                            analysis['volatility'],
                            analysis['trend'],
                            analysis['support_level'],
                            analysis['resistance_level'],
                            analysis['volume_ratio'],
                            json.dumps(analysis['technical_indicators']),
                            json.dumps(analysis['polynomial_regression']),
                            json.dumps(analysis['signal_analysis']),
                            analysis['data_points'],
                            analysis['analysis_period_days']
                        ))
                        logger.info(f"Inserted new analytics for {symbol}")
                    
                    ingested_count += 1
            
            self.connection.commit()
            logger.info(f"Successfully ingested {ingested_count} asset analytics")
            return ingested_count
            
        except Exception as e:
            logger.error(f"Error ingesting asset analytics: {e}")
            self.connection.rollback()
            return 0
    
    def cleanup_old_data(self, days: int = 30):
        """
        Clean up old data to prevent database bloat
        
        Args:
            days: Number of days to keep data
        """
        try:
            with self.connection.cursor() as cursor:
                # Clean up old alerts
                cursor.execute("""
                    DELETE FROM trading.alerts 
                    WHERE created_at < NOW() - INTERVAL '%s days'
                """, (days,))
                alerts_deleted = cursor.rowcount
                
                # Clean up old analytics
                cursor.execute("""
                    DELETE FROM trading.asset_analytics 
                    WHERE created_at < NOW() - INTERVAL '%s days'
                """, (days,))
                analytics_deleted = cursor.rowcount
                
                # Clean up old summaries (keep only last 7 days)
                cursor.execute("""
                    DELETE FROM trading.analysis_summary 
                    WHERE created_at < NOW() - INTERVAL '7 days'
                """, ())
                summaries_deleted = cursor.rowcount
                
                self.connection.commit()
                logger.info(f"Cleanup completed: {alerts_deleted} alerts, {analytics_deleted} analytics, {summaries_deleted} summaries deleted")
                
        except Exception as e:
            logger.error(f"Error during cleanup: {e}")
            self.connection.rollback()
    
    def ingest_latest_results(self) -> Dict:
        """
        Ingest the latest analysis results
        
        Returns:
            Dictionary with ingestion results
        """
        try:
            # Connect to database
            self.connect_database()
            
            # Create tables if they don't exist
            self.create_tables_if_not_exist()
            
            # Get latest results file
            latest_file = self.get_latest_results_file()
            if not latest_file:
                return {'error': 'No results files found'}
            
            # Load results
            results = self.load_results(latest_file)
            if not results:
                return {'error': 'Failed to load results'}
            
            # Ingest data
            alerts_ingested = self.ingest_alerts(results)
            summary_ingested = self.ingest_analysis_summary(results)
            analytics_ingested = self.ingest_asset_analytics(results)
            
            # Cleanup old data
            self.cleanup_old_data()
            
            # Close database connection
            self.close_database()
            
            return {
                'success': True,
                'file_processed': latest_file,
                'alerts_ingested': alerts_ingested,
                'summary_ingested': summary_ingested,
                'analytics_ingested': analytics_ingested,
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error during ingestion: {e}")
            if self.connection:
                self.close_database()
            return {'error': str(e)}

def main():
    """Main function for running the ingestion system"""
    import argparse
    
    parser = argparse.ArgumentParser(description='Ingest local analysis results into database')
    parser.add_argument('--config', default='config.json', help='Configuration file path')
    parser.add_argument('--results-dir', default='results', help='Results directory path')
    parser.add_argument('--cleanup-days', type=int, default=30, help='Days to keep data')
    
    args = parser.parse_args()
    
    # Load configuration
    try:
        with open(args.config, 'r') as f:
            config = json.load(f)
    except Exception as e:
        print(f"Error loading config: {e}")
        return
    
    # Database configuration
    db_config = {
        'host': config.get('database_host', 'localhost'),
        'port': config.get('database_port', 5432),
        'database': config.get('database_name', 'autonama'),
        'user': config.get('database_user', 'postgres'),
        'password': config.get('database_password', 'postgres')
    }
    
    # Initialize and run ingestion
    ingestion_system = IngestionSystem(db_config, args.results_dir)
    results = ingestion_system.ingest_latest_results()
    
    if 'error' in results:
        print(f"‚ùå Ingestion failed: {results['error']}")
    else:
        print(f"‚úÖ Ingestion completed successfully!")
        print(f"üìÅ File processed: {results['file_processed']}")
        print(f"üìä Alerts ingested: {results['alerts_ingested']}")
        print(f"üìà Analytics ingested: {results['analytics_ingested']}")
        print(f"üìã Summary ingested: {results['summary_ingested']}")

if __name__ == "__main__":
    main() 
 
 